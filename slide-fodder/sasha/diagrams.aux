\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Logistic Regression for $N=T=100$. {\bf  Left}: For SGD test loss, larger learning rates give diminishing returns. {\bf  Middle}: Covariance indeed predicts SGD generalization gap. {\bf  Right}: SGD outperforms GD, as predicted by covariance data. \relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Neural Network (two tanh hidden layers of size 25) for $N=T=10$. We trained faster (in the wee hours before Sasha's group meet) by cutting each MNIST image to just its 14th column. This reduces 784 features to 28 features. {\bf  Left}: SGD test loss, polynomial approximation --- bad fit. {\bf  Middle}: SGD test loss, exponential approximation --- bad fit. {\bf  Right}: SGD generalization gap --- good fit, perhaps because higher-order terms cancel. \relax }}{5}}
